### brief summary
The paper concludes that reinforcement learning can significantly enhance the reasoning abilities of language models1. DeepSeek-R1-Zero demonstrates strong performance across various tasks using a pure RL approach without relying on cold-start data1. DeepSeek-R1 builds upon this by leveraging cold-start data alongside iterative RL fine-tuning, achieving performance comparable to OpenAI-o1-1217 on a range of tasks1. The paper also explores distilling reasoning capabilities into smaller dense models, using DeepSeek-R1 to generate 800K training samples for fine-tuning2. Results from these distilled models are promising, with some outperforming GPT-4o and Claude-3.5-Sonnet on math benchmarks2.

### influential part
The influential part of this paper is mainly on 
### improvements?

### discussion?

