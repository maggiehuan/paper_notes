Paper Reading in Feb 6
### brief summary

### influential part

### improvements?

### discussion?

LLM trained in two stages
1. unsupervised pretraining -> general-purpose representations
2. large scale instruction tuning and RL -> alignment
