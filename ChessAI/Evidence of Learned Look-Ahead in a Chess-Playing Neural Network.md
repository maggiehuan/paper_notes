This paper investigates **whether neural networks, specifically the policy network of Leela Chess Zero, learn to implement algorithms like look-ahead**. The authors focus on Leela’s performance in complex chess positions to determine if its impressive abilities stem from learned reasoning algorithms or simply vast collections of heuristics.

The paper focuses on three key findings to support the presence of learned look-ahead in Leela:
==================================================================

- **The authors provide evidence that neural networks, like Leela Chess Zero, can learn algorithms involving look-ahead in complex scenarios.** They demonstrate that this occurs "in the wild," meaning that these networks are not explicitly trained to showcase such mechanisms.
    
    - The researchers prove this by demonstrating that Leela has internal representations of future moves that are crucial for its performance. One key piece of evidence is the **unusual importance of activations on the target square of the third move in the optimal line of play (the principal variation).** Through **activation patching,** a technique for measuring the causal importance of specific model components, the authors found that replacing activations on this specific square with activations from a slightly altered board state significantly reduces Leela’s performance. This suggests that Leela is storing crucial information about the third move on this square.
    - Further evidence comes from the analysis of attention heads, which reveals that Leela has **learned to move information “forward and backward in time.”** The authors identify a specific attention head, **L12H12,** that often moves information from the target square of the third move to the target square of the first move. **Zero-ablating** this single entry in the attention pattern—effectively blocking this information pathway—significantly impacts Leela’s performance, indicating its importance in the decision-making process.
    - Finally, the authors trained a simple **bilinear probe** to predict the optimal move two turns into the future. This probe, which utilizes a subset of Leela’s activations, achieves an accuracy of 92%, demonstrating that the network explicitly encodes information about future moves. The probe architecture is inspired by observations on the attention patterns of L12H12, highlighting the connection between this specific head and the network’s ability to perform look-ahead.
- **The authors offer insights into the potential mechanisms of how look-ahead might be implemented in Leela.**
    
    - Beyond the role of L12H12 in moving information backward in time, the authors identified **"piece movement heads,"** which appear to assist Leela in analyzing the consequences of future moves. These heads exhibit attention patterns that resemble legal moves for specific piece types. For instance, "knight heads" focus on squares that a knight could reach. By selectively **ablating information flow** out of the target square of the third move in these piece movement heads, the authors observed a significant drop in Leela’s performance. This finding supports their hypothesis that these heads contribute to analyzing future move consequences.
- **The authors introduce techniques that could be useful for the broader field of interpretability.** These techniques include:
    
    - Using a weaker model to automatically generate “interesting” corruptions for activation patching.
    - Analyzing information flow through attention heads by zero-ablating specific entries in the attention pattern.
    - Employing bilinear probes to predict future moves based on a subset of activations.

The authors emphasize that their work provides a starting point for understanding how look-ahead might be implemented in neural networks, and further research is necessary to gain a more precise and complete picture of these mechanisms.

The authors acknowledge that their findings don't fully explain the exact algorithm Leela uses for look-ahead and that their focus on complex chess positions might not represent how Leela operates across all situations. They believe that their research, which uses common interpretability techniques, provides evidence of complex algorithmic mechanisms in neural networks, which could be valuable for future studies on the capabilities and potential risks of advanced AI systems like large language models.