This paper investigates **whether neural networks, specifically the policy network of Leela Chess Zero, learn to implement algorithms like look-ahead**. The authors focus on Leela’s performance in complex chess positions to determine if its impressive abilities stem from learned reasoning algorithms or simply vast collections of heuristics.

The paper focuses on three key findings to support the presence of learned look-ahead in Leela:
- **Activations on the target square of the move two turns into the future (the 3rd move) are unusually important for Leela’s output.** This is determined through activation patching, where activations on certain squares are replaced with those from a slightly modified board state.
- **Specific attention heads seem to play a crucial role in Leela’s look-ahead process.** One attention head, L12H12, appears to transfer information from the 3rd move target square “backward in time” to the 1st move target square. Other attention heads, called “piece movement heads,” seem to analyze the consequences of future moves by focusing on squares reachable by specific piece types.
- **A simple probe, inspired by the function of L12H12, can predict the optimal move two turns into the future with 92% accuracy.** This probe uses a bilinear form to predict the target and source squares of the 3rd move based on the activations of specific squares.

The authors acknowledge that their findings don't fully explain the exact algorithm Leela uses for look-ahead and that their focus on complex chess positions might not represent how Leela operates across all situations. They believe that their research, which uses common interpretability techniques, provides evidence of complex algorithmic mechanisms in neural networks, which could be valuable for future studies on the capabilities and potential risks of advanced AI systems like large language models.