# This is Maggie's paper reading list. 
All the paper and its link will be added here. And notes are in the files. Most papers are project-related, some might be interest-based or nice/trendy paper that I enjoyed. 

## Planning - from Jan 1, 2025
1. Expertise increases planning depth in human gameplay https://www.nature.com/articles/s41586-023-06124-2 [[Expertise increases planning depth in human gameplay]]

## Chess with AI - from Jan 1, 2025
1. [[Amortized Planning with Large-Scale Transformers A Case Study on Chess]],  https://arxiv.org/abs/2402.04494,
2. [[The Chess Transformer Mastering Play using Generative Language Models]] https://arxiv.org/pdf/2008.04057
3. [[Mastering Chess with a Transformer Model]] https://arxiv.org/pdf/2409.12272
4. [[Evidence of Learned Look-Ahead in a Chess-Playing Neural Network]] https://arxiv.org/pdf/2406.00877v1
5. [[Transcendence Generative Models Can Outperform The Experts That Train Them]] https://arxiv.org/abs/2406.11741v3
6. [[Grandmaster-Level Chess Without Search]] https://arxiv.org/pdf/2402.04494v1 
7. Skill in Chess https://iiif.library.cmu.edu/file/Simon_box00066_fld05052_bdl0001_doc0001/Simon_box00066_fld05052_bdl0001_doc0001.pdf
8. 

## Good Papers
### Nice "old"  paper
#### Machine Learning
- An Empirical Model of Large-Batch Training https://arxiv.org/pdf/1812.06162
- Neural Turing Machines https://arxiv.org/pdf/1410.5401
- Variational Lossy Auto-encoder https://arxiv.org/pdf/1611.02731
#### NLP
##### Early Systems
- Understanding Natural Language https://www.sciencedirect.com/science/article/abs/pii/0010028572900023
- 
##### Language Models
- Improving Neural Language Models with A Continuous Cache https://arxiv.org/pdf/1612.04426
- Exploring the Limits of Language Modeling https://arxiv.org/pdf/1602.02410
- Recurrent neural network based language model https://www.fit.vut.cz/research/group/speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf
- Attention, Intentions, and the Structure of Discourse https://aclanthology.org/J86-3001.pdf


### Interesting Ones
- Using games to understand the mind
- People construct simplified mental representations to plan

### Others
With notes:
- Better & Faster Large Language Models via Multi-token Prediction https://arxiv.org/pdf/2404.19737
- Large Concept Models: Language Modeling in a Sentence Representation Space https://arxiv.org/abs/2412.08821
- 
W/o notes:

### Readings for CIS 6300
- Itâ€™s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners https://arxiv.org/pdf/2009.07118
- Language Models are Few-Shot Learners https://arxiv.org/abs/2005.14165
- How Many Data Points is a Prompt Worth? https://arxiv.org/abs/2103.08493
- Rethinking the Role of Demonstrations What Makes In-Context Learning Work? https://arxiv.org/abs/2202.12837
- Larger language models do in-context learning differently https://arxiv.org/abs/2303.03846
- 

